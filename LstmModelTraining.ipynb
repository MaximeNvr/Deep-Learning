{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3686e0d-e1e6-450f-a331-f9ed2e9eb826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 16:29:50.676375: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-18 16:29:50.717925: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-18 16:29:51.362708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eee7c96-5ddd-4d13-b58d-b407f864bdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, TensorFlow!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 16:29:52.123146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-18 16:29:52.172308: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a constant tensor\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "\n",
    "# Run the TensorFlow session\n",
    "tf.print(hello)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08c2e56-b255-481d-8d9a-0b8b05313153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  generated\n",
       "0  Cars. Cars have been around since they became ...        0.0\n",
       "1  Transportation is a large necessity in most co...        0.0\n",
       "2  \"America's love affair with it's vehicles seem...        0.0\n",
       "3  How often do you ride in a car? Do you drive a...        0.0\n",
       "4  Cars are a wonderful thing. They are perhaps o...        0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('AI_Human.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f561dc79-9b4e-4cc6-b3b9-5078a7b2a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openpyxl installed and working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
    "from keras.preprocessing.sequence import pad_sequences  # For padding sequences\n",
    "from keras.models import Sequential  # For creating a sequential model\n",
    "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D  # For building LSTM model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import openpyxl\n",
    "print(\"openpyxl installed and working correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b97a40b7-5a5e-4d66-b9b9-24a8715b2d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "texts = df.iloc[:, 0].values  # The text column\n",
    "labels = df.iloc[:, 1].values  # The labels column (0 for human, 1 for AI)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Separate features and labels\n",
    "texts = df.iloc[:, 0].values  # The text column\n",
    "labels = df.iloc[:, 1].values  # The labels column (0 for human, 1 for AI)\n",
    "\n",
    "# Split the data into training and temp sets (80% train, 20% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temp set into validation and test sets (50% of temp for each, so 10% of original)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b37ef-cc75-46a2-a7d3-6681105ea111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.3, 64, 2000)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models:   0%|                                   | 0/5 [00:00<?, ?run/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination: Dropout=0.1, Learning Rate=0.3, Batch Size=64, Vocab Size=2000\n",
      "Tokenizer saved to tokenizer.pkl\n",
      "Fin de la tokenisation\n",
      "entrainement du model :  0.1 0.3 64 2000\n",
      "Epoch 1/5\n",
      "6091/6091 [==============================] - 2855s 469ms/step - loss: 0.0840 - accuracy: 0.9723 - val_loss: 0.0161 - val_accuracy: 0.9954\n",
      "Epoch 2/5\n",
      "6091/6091 [==============================] - 2720s 447ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0081 - val_accuracy: 0.9974\n",
      "Epoch 3/5\n",
      "6091/6091 [==============================] - 2731s 448ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0050 - val_accuracy: 0.9986\n",
      "Epoch 4/5\n",
      "6091/6091 [==============================] - 2738s 449ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0040 - val_accuracy: 0.9990\n",
      "Epoch 5/5\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9989"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "model_path = \"monModel.h5\"\n",
    "\n",
    "def train_model(dropout, learning_rate, batch_size, vocab_size, X_train_pad, X_val_pad, X_test_pad, y_train, y_val, y_test):\n",
    "    # Building the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=dropout, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with specified learning rate\n",
    "    model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_pad, y_train, \n",
    "                        validation_data=(X_val_pad, y_val), \n",
    "                        epochs=10,  # Adjust the number of epochs as needed\n",
    "                        batch_size=batch_size, \n",
    "                        verbose=1)  # Display training progress\n",
    "    \n",
    "     # Save the model\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "\n",
    "    # Get validation accuracy\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "    return val_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "# Paramètres\n",
    "dropout_list = [0,0.1]\n",
    "learning_rate_list = [0.3]\n",
    "batch_size_list = [64,128]\n",
    "number_of_token_in_vocab = [500,1000,2000]\n",
    "excel_file = \"model_accuracy_results.xlsx\"\n",
    "\n",
    "# Déterminer toutes les combinaisons possibles\n",
    "all_combinations = list(itertools.product(dropout_list, learning_rate_list, batch_size_list, number_of_token_in_vocab))\n",
    "\n",
    "# Lire le fichier Excel s'il existe\n",
    "if os.path.exists(excel_file):\n",
    "    existing_results = pd.read_excel(excel_file)\n",
    "    completed_combinations = set(zip(\n",
    "        existing_results['Dropout'],\n",
    "        existing_results['Learning Rate'],\n",
    "        existing_results['Batch Size'],\n",
    "        existing_results['Vocab Size']\n",
    "    ))\n",
    "else:\n",
    "    existing_results = pd.DataFrame()\n",
    "    completed_combinations = set()\n",
    "\n",
    "# Filtrer les combinaisons restantes\n",
    "remaining_combinations = [comb for comb in all_combinations if comb not in completed_combinations]\n",
    "print(remaining_combinations)\n",
    "\n",
    "# Initialiser la barre de progression\n",
    "with tqdm(total=len(remaining_combinations) * 5, desc=\"Training Models\", unit=\"run\") as pbar:\n",
    "    for (dropout, learning_rate, batch_size, vocab_size) in remaining_combinations:\n",
    "        print(f\"Running combination: Dropout={dropout}, Learning Rate={learning_rate}, Batch Size={batch_size}, Vocab Size={vocab_size}\")\n",
    "        val_accuracies = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        # Tokenizing the text data\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        # Save the tokenizer for later use\n",
    "        with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(tokenizer, f)\n",
    "        print(\"Tokenizer saved to tokenizer.pkl\")\n",
    "        X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "        X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "        X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "        print(\"Fin de la tokenisation\")\n",
    "        max_sequence_length = 1000\n",
    "        X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "        X_val_pad = pad_sequences(X_val_seq, maxlen=max_sequence_length)\n",
    "        X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
    "\n",
    "        # Exécuter chaque combinaison 5 fois\n",
    "        for i in range(5):\n",
    "            print(\"entrainement du model : \", dropout, learning_rate, batch_size, vocab_size)\n",
    "            val_accuracy, test_accuracy = train_model(\n",
    "                dropout, learning_rate, batch_size, vocab_size,\n",
    "                X_train_pad, X_val_pad, X_test_pad,\n",
    "                y_train, y_val, y_test\n",
    "            )\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            pbar.update(1)\n",
    "\n",
    "        avg_val_accuracy = np.mean(val_accuracies)\n",
    "        avg_test_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "        # Ajouter les résultats pour cette combinaison\n",
    "        new_result = {\n",
    "            'Dropout': dropout,\n",
    "            'Learning Rate': learning_rate,\n",
    "            'Batch Size': batch_size,\n",
    "            'Vocab Size': vocab_size,\n",
    "            'Average Validation Accuracy': avg_val_accuracy,\n",
    "            'Average Test Accuracy': avg_test_accuracy\n",
    "        }\n",
    "\n",
    "        # Ajouter au DataFrame existant\n",
    "        existing_results = pd.concat([existing_results, pd.DataFrame([new_result])], ignore_index=True)\n",
    "\n",
    "        # Sauvegarder immédiatement dans le fichier Excel\n",
    "        existing_results.to_excel(excel_file, index=False)\n",
    "\n",
    "print(\"All results saved to 'model_accuracy_results.xlsx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
